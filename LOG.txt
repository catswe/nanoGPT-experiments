14.
inherit: 9
step 2000: train loss 1.5101, val loss 1.6946
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.5130, time 401.55ms, mfu 0.05%
@@ -79,21 +79,17 @@ class MLP(nn.Module):
 
     def __init__(self, config):
         super().__init__()
-        self.ln1 = LayerNorm(config.n_embd * 2, bias=config.bias)
         self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
         self.gelu    = nn.GELU()
-        self.ln2 = LayerNorm(4 * config.n_embd, bias=config.bias)
         self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
-        self.dropout = nn.Dropout(config.dropout)
 
     def forward(self, x):
-        two_split = self.ln1(torch.cat([self.gelu(x), self.gelu(-x)], dim=-1))
+        two_split = F.layer_norm(torch.cat([self.gelu(x), self.gelu(-x)], dim=-1), [2 * x.size(-1)])
         four_split = torch.cat([self.gelu(two_split), self.gelu(-two_split)], dim=-1)
 
         x = self.c_fc(x)
         x = self.gelu(x) + four_split
-        x = self.c_proj(self.ln2(x))
-        x = self.dropout(x)
+        x = self.c_proj(F.layer_norm(x, [x.size(-1)]))
         return x

13.
inherit: 11
step 2000: train loss 1.5768, val loss 1.7484
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.5433, time 285.46ms, mfu 0.07%
learning_rate = 1e-3 * 3 # with baby networks can afford to go a bit higher

12.
inherit: 11
step 2000: train loss 1.5593, val loss 1.7277
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.5380, time 258.27ms, mfu 0.07%
learning_rate = 1e-3 * 4 # with baby networks can afford to go a bit higher

11.
inherit: 9
step 2000: train loss 1.5740, val loss 1.7361
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.5543, time 280.23ms, mfu 0.07%
@@ -81,17 +81,17 @@ class MLP(nn.Module):
         super().__init__()
         self.ln1 = LayerNorm(config.n_embd * 2, bias=config.bias)
         self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
-        self.gelu    = nn.GELU()
+        self.activation = lambda x: torch.relu(x)
         self.ln2 = LayerNorm(4 * config.n_embd, bias=config.bias)
         self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
         self.dropout = nn.Dropout(config.dropout)
 
     def forward(self, x):
-        two_split = self.ln1(torch.cat([self.gelu(x), self.gelu(-x)], dim=-1))
-        four_split = torch.cat([self.gelu(two_split), self.gelu(-two_split)], dim=-1)
+        two_split = self.ln1(torch.cat([self.activation(x), self.activation(-x)], dim=-1))
+        four_split = torch.cat([self.activation(two_split), self.activation(-two_split)], dim=-1)
 
         x = self.c_fc(x)
-        x = self.gelu(x) + four_split
+        x = self.activation(x) + four_split
         x = self.c_proj(self.ln2(x))

10.
inherit: 8
step 2000: train loss 1.5211, val loss 1.6945
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.5139, time 467.29ms, mfu 0.05%
-learning_rate = 1e-3 * 5 # with baby networks can afford to go a bit higher
+learning_rate = 1e-3 * 6 # with baby networks can afford to go a bit higher

9.
inherit: 8
step 2000: train loss 1.5061, val loss 1.6950
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.4948, time 442.20ms, mfu 0.05%
-learning_rate = 1e-3 * 4 # with baby networks can afford to go a bit higher
+learning_rate = 1e-3 * 5 # with baby networks can afford to go a bit higher

8.
inherit: 6
step 2000: train loss 1.5123, val loss 1.7027
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.4963, time 471.93ms, mfu 0.05%
@@ -79,19 +79,20 @@ class MLP(nn.Module):
 
     def __init__(self, config):
         super().__init__()
-        self.ln = LayerNorm(config.n_embd * 2, bias=config.bias)
+        self.ln1 = LayerNorm(config.n_embd * 2, bias=config.bias)
         self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
         self.gelu    = nn.GELU()
+        self.ln2 = LayerNorm(4 * config.n_embd, bias=config.bias)
         self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
         self.dropout = nn.Dropout(config.dropout)
 
     def forward(self, x):
-        two_split = self.ln(torch.cat([self.gelu(x), self.gelu(-x)], dim=-1))
+        two_split = self.ln1(torch.cat([self.gelu(x), self.gelu(-x)], dim=-1))
         four_split = torch.cat([self.gelu(two_split), self.gelu(-two_split)], dim=-1)
 
         x = self.c_fc(x)
         x = self.gelu(x) + four_split
         x = self.c_proj(self.ln2(x))
         x = self.dropout(x)
         return x

7.
inherit: 5, 6
step 2000: train loss 1.5243, val loss 1.7034
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.5084, time 347.88ms, mfu 0.05%

6.
inherit: 4
step 2000: train loss 1.5127, val loss 1.6887
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.4962, time 354.25ms, mfu 0.05%
@@ -79,14 +79,18 @@ class MLP(nn.Module):
 
     def __init__(self, config):
         super().__init__()
+        self.ln = LayerNorm(config.n_embd * 2, bias=config.bias)
         self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
         self.gelu    = nn.GELU()
         self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
         self.dropout = nn.Dropout(config.dropout)
 
     def forward(self, x):
+        two_split = self.ln(torch.cat([self.gelu(x), self.gelu(-x)], dim=-1))
+        four_split = torch.cat([self.gelu(two_split), self.gelu(-two_split)], dim=-1)
+
         x = self.c_fc(x)
-        x = self.gelu(x)
+        x = self.gelu(x) + four_split
         x = self.c_proj(x)
         x = self.dropout(x)
         return x
@@ -144,6 +148,15 @@ class GPT(nn.Module):
             if pn.endswith('c_proj.weight'):
                 torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))
 
+        for block in self.transformer.h:
+            torch.nn.init.zeros_(block.mlp.c_fc.weight)
+            torch.nn.init.zeros_(block.mlp.c_proj.weight)
+
+            if block.mlp.c_fc.bias is not None:
+                torch.nn.init.zeros_(block.mlp.c_fc.bias)
+            if block.mlp.c_proj.bias is not None:
+                torch.nn.init.zeros_(block.mlp.c_proj.bias)
+
         # report number of parameters
         print("number of parameters: %.2fM" % (self.get_num_params()/1e6,))

5.
inherit: 1
step 2000: train loss 1.6086, val loss 1.7693
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.5574, time 230.04ms, mfu 0.08%
-learning_rate = 1e-3 # with baby networks can afford to go a bit higher
+learning_rate = 1e-3 * 5 # with baby networks can afford to go a bit higher

4.
inherit: 1
step 2000: train loss 1.5904, val loss 1.7608
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.5595, time 226.91ms, mfu 0.08%
-learning_rate = 1e-3 # with baby networks can afford to go a bit higher
+learning_rate = 1e-3 * 4 # with baby networks can afford to go a bit higher

3.
inherit: 1
step 2000: train loss 1.5937, val loss 1.7553
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.5477, time 203.18ms, mfu 0.08%
-learning_rate = 1e-3 # with baby networks can afford to go a bit higher
+learning_rate = 1e-3 * 3 # with baby networks can afford to go a bit higher

2.
inherit: 1
step 2000: train loss 1.6308, val loss 1.7877
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.5736, time 221.89ms, mfu 0.08%
-learning_rate = 1e-3 # with baby networks can afford to go a bit higher
+learning_rate = 1e-3 * 2 # with baby networks can afford to go a bit higher

1. Original code
step 2000: train loss 1.7648, val loss 1.8857
saving checkpoint to out-shakespeare-char
iter 2000: loss 1.6958, time 199.32ms, mfu 0.09%